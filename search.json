[{"title":"Redis知识点","path":"/2024/06/09/Redis知识点/","content":"Redis数据结构和类型 Redis有5种数据类型：字符串、链表、哈希、集合、有序集合 与8种数据结构分别对应： String List Hash Set ZSet SDS（简单动态字符串） LinkedList、ZipList、QuickList Dict、ZipList Dict、IntSet ZipList、SkipList String 应用场景： 需要存储常规数据的场景：Session、Token、图片地址、序列化后的对象 需要计数的场景：用户单位时间请求数、页面单位时间访问量 分布式锁：SETNX KEY VALUE List 应用场景： 信息流展示：最新文章、最新动态相关命令：LPUSH、LRANGE 消息队列，不建议用Redis实现消息队列 Hash Redis的Hash是一个 String 类型的 field-value 映射表，特别适合存储对象。 应用场景： 对象存储：用户信息、商品信息、文章信息、购物车信息 存对象用String还是Hash？ String 存的是序列化后的对象，存的是整个对象。Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改、添加字段，节省网络流量。如果对象中的某个字段信息需要经常改动，或者经常查找，用 Hash 更合适。 String 更加节省内存，缓存相同数据量的对象，String 的内存消耗约是 Hash 的一半，并且，存储具有多层嵌套的对象也更加方便。如果系统对性能和资源消耗比较敏感，用 String 就更加合适。 Set 元素无序且唯一，Set提供了多个判断元素是否存在于集合中的接口，List不具备该接口 可以利用Set进行交集、并集、差集操作，可以实现共同关注、共同粉丝等功能。 常用指令： 应用场景： **存放的数据不能重复的场景：**UV统计（统计量大的话，还是HyperLogLog更合适）、点赞信息等 相关指令：SCARD（获取集合元素数量） 需要获取多个数据源的交集、并集、差集信息时 如：共同好友(交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集+交集） 相关指令：SINTER（交集）、SINTERSTORE （交集）、SUNION （并集）、SUNIONSTORE（并集）、SDIFF（差集）、SDIFFSTORE （差集） 需要随机获取数据源中的元素时，如抽奖、点名 相关指令：SPOP（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、SRANDMEMBER（随机获取集合中的元素，适合允许重复中奖的场景 ZSet ZSet相比于Set，增加里一个权重参数score，使集合中的元素可以按照权重排序，还可以通过权重范围获取元素列表 Redis有序集合的底层： ZSet的底层是跳表和压缩链表ziplist，为什么不是平衡树、红黑树、B+树？ 平衡树 vs 跳表：平衡树的插入、删除和查询的时间复杂度和跳表一样都是 O(log n)。对于范围查询来说，平衡树也可以通过中序遍历的方式达到和跳表一样的效果。但是它的每一次插入或者删除操作都需要保证整颗树左右节点的绝对平衡，只要不平衡就要通过旋转操作来保持平衡，这个过程是比较耗时的。跳表诞生的初衷就是为了克服平衡树的一些缺点。跳表使用概率平衡而不是严格强制的平衡，因此，跳表中的插入和删除算法比平衡树的等效算法简单得多，速度也快得多。 **红黑树 vs 跳表：**相比较于红黑树来说，跳表的实现也更简单一些，不需要通过旋转和染色（红黑变换）来保证黑平衡。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高。 B+树 vs 跳表： B+树更适合作为数据库和文件系统中常用的索引结构之一，它的核心思想是通过可能少的 IO 定位到尽可能多的索引来获得查询数据。对于 Redis 这种内存数据库来说，它对这些并不感冒，因为 Redis 作为内存数据库它不可能存储大量的数据，所以对于索引不需要通过 B+树这种方式进行维护，只需按照概率进行随机维护即可，节约内存。而且使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并 我们通过 object 指令查看 zset 的数据结构，可以看到当前有序集合存储的还是是ziplist(压缩列表)。 127.0.0.1:6379&gt; object encoding rankList&quot;ziplist&quot; 设计者考虑到 Redis 数据存放于内存，为了节约宝贵的内存空间在有序集合在元素小于 64 字节且个数小于 128 的时候，会使用 ziplist，而这个阈值的默认值的设置就来自下面这两个配置项。 zset-max-ziplist-value 64zset-max-ziplist-entries 128 一旦有序集合中的某个元素超出这两个其中的一个阈值它就会转为 skiplist（实际是 dict+skiplist，还会借用字典来提高获取指定元素的效率。 应用场景： 需要随机获取数据源中的元素根据某个权重排序的场景相关指令：ZRANGE (从小到大排序)、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 需要存储的元素有优先级或重要程度：如优先级队列 Bitmap Bitmap不是Redis的基本数据类型，而是在String类型上定义的一组面向位的操作，将其视为位向量。由于字符串是二进制安全的块，且最大长度为 512 MB，它们适合用于设置最多 2^32 个不同的位。 可以将Bitmap看作一个二进制数字（0，1）的数组，数组中每个元素的下标叫做offset（偏移量）。 常用指令： 应用场景： 需要保存状态信息的场景，如：用户签到，活跃用户情况，用户行为统计（是否浏览过某资源） 如果想要使用 Bitmap 统计活跃用户的话，可以使用日期（精确到天）作为 key，然后用户 ID 为 offset，如果当日活跃过就设置为 1。 HyperLogLog(基数统计) 使用 HyperLogLog 统计页面 UV 主要需要用到下面这两个命令： PFADD key element1 element2 ...：添加一个或多个元素到 HyperLogLog 中。 PFCOUNT key1 key2：获取一个或者多个 HyperLogLog 的唯一计数。 Geospatial(地理位置) 基于Sorted Set实现，主要用于存储地理位置 常用命令： Redis线程模型 Redis单线程模型 对于读写命令，redis 一直是单线程模型，不过在 4.0 版本以后引入了多线程来执行一些大键值对的异步操作，在 6.0 后引入了多线程来处理网络请求，提高网络 IO 读写性能。 Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型。( Netty 的线程模型也是基于该模式 ) 这套事件处理模型对应的是 Redis 中的文件事件处理器（file event handler） 由于文件事件处理器（file event handler）是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。 文件事件处理器使用 I/O 多路复用程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。 既然是单线程，如何监听大量的客户端连接呢？ Redis 通过 IO 多路复用程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。 I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗（和 NIO 中的 Selector 组件很像）。 文件事件处理器包括以下部分： 多个 socket (客户端连接) IO 多路复用程序（支持多个客户端连接的关键） 文件事件分派器（将 socket 关联到相应的事件处理器） 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 4.0 版本的多线程： Redis 4.0 增加的多线程主要是针对一些大键值对的删除操作的命令，使用这些命令就会使用主线程之外的其他线程来**“异步处理”，从而减少对主线程的影响。为此，Redis 4.0 之后新增了几个异步命令：** UNLINK：可以看作是 DEL 命令的异步版本。 FLUSHALL ASYNC：用于清空所有数据库的所有键，不限于当前 SELECT 的数据库。 FLUSHDB ASYNC：用于清空当前 SELECT 数据库中的所有键。 总的来说，直到 Redis 6.0 之前，Redis 的主要操作仍然是单线程处理的。 那 Redis6.0 之前为什么不使用多线程？ 单线程编程容易并且更容易维护； Redis 的性能瓶颈不在 CPU ，主要在内存和网络； 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。 6.0 版本新增的多线程 Redis6.0 引入多线 程主要是为了提高网络 IO 读写性能，因为这个算是 Redis 中的一个性能瓶颈**（Redis 的瓶颈主要受限于内存和网络）** 虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。 Redis后台线程 我们虽然经常说 Redis 是单线程模型（主要逻辑是单线程完成的），但实际还有一些后台线程用于执行一些比较耗时的操作： 通过 bio_close_file 后台线程来释放 AOF / RDB 等过程中产生的临时文件资源。 通过 bio_aof_fsync 后台线程调用 fsync 函数将系统内核缓冲区还未同步到到磁盘的数据强制刷到磁盘（ AOF 文件）。 通过 bio_lazy_free后台线程释放大对象（已删除）占用的内存空间. Redis为什么这么快？ Redis 基于内存，内存的访问速度比磁盘快很多； Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型，主要是单线程事件循环和 IO 多路复用（Redis 线程模式后面会详细介绍到）； Redis 内置了多种优化过后的数据类型/结构实现，性能非常高。 Redis 通信协议实现简单且解析高效。 那既然都这么快了，为什么不直接用 Redis 当主数据库呢？主要是因为内存成本太高且 Redis 提供的数据持久化仍然有数据丢失的风险。 虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。 为什么用Redis? 访问速度快 高并发 一般像 MySQL 这类的数据库的 QPS 大概都在 4k 左右（4 核 8g） ，但是使用 Redis 缓存之后很容易达到 5w+，甚至能达到 10w+（就单机 Redis 的情况，Redis 集群的话会更高）。 QPS（Query Per Second）：服务器每秒可以执行的查询次数； 功能全面除了做缓存，redis还可以做分布式锁、限流、消息队列、延时队列、分布式Session等 Redis缓存读写策略 旁路缓存模式（Cache Aside Pattern) 人工编码方式，缓存调用者在更新完数据库后再去更新缓存，也成为双写方案。 服务器同时维护cache和db，并且是以db的结果为准 比较适合请求比较多的场景。 ​ 缓存读写步骤：​ 写：先更新DB，再删除缓存​ 读：从cache中读取缓存，读取到就直接返回；未读取到就查DB，从DB中直接返回，然后重建缓存 ​ 先更新数据库，再删除缓存，也是有可能出现双写不一致性问题的，比如：当前缓存不存在目标数据，线程A查询缓存，未命中缓存，然后线程A查数据库，查到数据后，在重构缓存之前，切换到线程B，线程B修改数据库目标数据，然后删除缓存，此时切换到线程A，线程A重构缓存，这样就出现了缓存和数据库不一致的问题。 旁路缓存模式的缺陷： 首次请求数据一定不在cache中 解决方法：缓存预热 写操作频繁的话，会导致cache中的数据被频繁删除，影响缓存命中率解决方法：（1）采用db和cache强一致性：更新db时，保证cache更新，需要加锁来完成（分布式锁）（2）短暂地允许cache和db不一致，更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。 读写穿透模式（Read/Write Through Pattern) ​\t把cache视为主要数据存储，对其进行读写，cache服务负责将此数据写入db中，从而减轻了应用程序的职责。 ​\t用的较少，我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入 db 的功能。 ​\t缓存读写步骤：​\t写：先查cache，未命中则直接更新db；若命中，则先更新cache，然后cache服务自己更新db​\t读：从cache中读取数据，命中直接返回；未命中则从db加载，先重建缓存后响应数据 异步缓存写入模式（Write Behind Pattern) ​\t调用者只操作缓存，其他线程去异步处理数据库，最终实现一致。 ​\tWrite Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 db 的读写。 ​\t但是，两个又有很大的不同：Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。 ​\t很明显，这种方式对数据一致性带来了更大的挑战，比如 cache 数据可能还没异步更新 db 的话，cache 服务可能就就挂掉了。 ​\t这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制都用到了这种策略。 ​\tWrite Behind Pattern 下 db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量 缓存三兄弟 缓存穿透 缓存穿透是指查询一个缓存和数据库都不存在的数据，这将导致这个不存在的数据每次请求都要到 DB 去查询，可能导致 DB 挂掉。这种情况大概率是遭到了攻击。 缓存空值 缓存预热是一种在应用程序启动或缓存失效之后，主动将热点数据加载到缓存中的策略。这样，在实际请求到达应用程序时，热点数据已经存在于缓存中，从而减少了缓存未命中的情况，提高了应用程序的响应速度。 布隆过滤器 布隆过滤器主要是用于检索一个元素是否在一个集合中。我们当时使用的是redisson实现的布隆过滤器。 它的底层主要是先去初始化一个比较大数组，里面存放的二进制0或1。在一开始都是0，当一个key来了之后经过3次hash计算，模于数组长度找到数据的下标然后把数组中原来的0改为1，这样的话，三个数组的位置就能标明一个key的存在。查找的过程也是一样的。 当然是有缺点的，布隆过滤器有可能会产生一定的误判，我们一般可以设置这个误判率，大概不会超过5%，其实这个误判是必然存在的，要不就得增加数组的长度，其实已经算是很划算了，5%以内的误判率一般的项目也能接受，不至于高并发下压倒数据库。 接口限流 根据用户或者 IP 对接口进行限流，对于异常频繁的访问行为，还可以采取黑名单机制，例如将异常 IP 列入黑名单。 后面提到的缓存击穿和雪崩都可以配合接口限流来解决，毕竟这些问题的关键都是有很多请求落到了数据库上造成数据库压力过大。 缓存击穿 互斥锁、逻辑过期 互斥锁： 因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了 逻辑过期：不在Redis数据中设置有效期属性，而是在value中添加逻辑有效期字段我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。具体为：当线程1读取缓存，发现缓存过期后，会申请得到互斥锁，然后开启一个线程来查询数据重构缓存，然后返回过期数据；在后台线程释放锁之前，其他线程读取到过期缓存后，没有拿到互斥锁并不会被阻塞，而是直接返回过期的脏数据。 优点是异步地构建缓存，缺点是缓存重构完成之前，其他线程返回的都是脏数据。 缓存雪崩 或者设置定时任务，更新过期时间 针对Redis服务宕机的情况 Redis 集群：采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。Redis Cluster 和 Redis Sentinel 是两种最常用的 Redis 集群实现方案 多级缓存：设置多级缓存，例如本地缓存+Redis 缓存的二级缓存组合，当 Redis 缓存出现问题时，还可以从本地缓存中获取到部分数据。 针对大量缓存同时失效的情况 设置随机失效时间（可选）：为缓存设置随机的失效时间，例如在固定过期时间的基础上加上一个随机值，这样可以避免大量缓存同时到期，从而减少缓存雪崩的风险。 提前预热（推荐）：针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。 持久缓存策略（看情况）：虽然一般不推荐设置缓存永不过期，但对于某些关键性和变化不频繁的数据，可以考虑这种策略、 缓存预热 常见的缓存预热方式有两种： 使用定时任务，比如 xxl-job，来定时触发缓存预热的逻辑，将数据库中的热点数据查询出来并存入缓存中。 使用消息队列，比如 Kafka，来异步地进行缓存预热，将数据库中的热点数据的主键或者 ID 发送到消息队列中，然后由缓存服务消费消息队列中的数据，根据主键或者 ID 查询数据库并更新缓存 双写一致性 一般数据库采用的是主从模式，主从数据库同步需要时间 要求强一致性：用读写锁 但效率低，为了提高效率，改用读写锁，读的时候上共享锁，此时其他线程对该数据只能读不能写，写的时候上排他锁，此时其他线程不能读也不能写 以下为redisson实现的读写锁 允许延迟一致：保证数据的最终一致，用异步通知 数据持久化 RDB snapshotting 快照 RDB Redis 可以通过创建快照来获得存储在内存里面的数据在 某个时间点 上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。 Redis 提供了两个命令来生成 RDB 快照文件： save : 同步保存操作，会阻塞 Redis 主线程； bgsave : fork 出一个子进程，子进程执行，不会阻塞 Redis 主线程，默认选项。 AOF 与快照持久化相比，AOF 持久化的实时性更好。默认情况下 Redis 没有开启 AOF（append only file）方式的持久化（Redis 6.0 之后已经默认是开启了），可以通过 appendonly 参数开启： 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到 AOF 缓冲区 server.aof_buf 中，然后再写入到 AOF 文件中（此时还在系统内核缓存区未同步到磁盘），最后再根据持久化方式（ fsync策略）的配置来决定何时将系统内核缓存区的数据同步到硬盘中的。 AOF的工作流程 命令追加（append）：将所有写命令追加到AOF缓冲区中 文件写入（write）：将AOF缓冲区的数据写入系统内核缓冲区 文件同步（fsync）：AOF缓冲区根据对应的持久化方式（fsync策略）向硬盘同步操作，这一步需要调用fsync函数（系统调用），fsync针对单个文件操作，对其进行强制硬盘同步，fsync 将阻塞直到写入硬盘完成后返回，保证了数据持久化。 文件重写（rewirte)：随着AOF文件越来越大，定时对AOF文件进行重写，达到压缩目的，由子线程执行 重新加载（road)：redis重启时，加载AOF文件进行数据恢复 刷盘策略（AOF持久化策略） appendfsync always：主线程调用==write==后，后台线程会立刻调用fsync函数同步AOF文件，fsync完成后线程返回，会严重降低redis的性能（write+fsync) appendfsync everysec：主线程调用write后，执行操作后立即返回，由后台线程每秒钟调用一次fsync来同步一次AOF文件(write+fsync，fsync间隔为1秒) appendfsync no：主线程调用 write 执行写操作后立即返回，让操作系统决定何时进行同步，Linux 下一般为 30 秒一次（write但不fsync，fsync 的时机由操作系统决定）。 记录日志的时机：命令执行完成之后。为什么？ 避免额外的检查开销，AOF记录日志不会对语法进行检查 在命令执行完之后再记录，不会阻塞当前命令的执行但也带来了风险： 如果刚执行完命令，redis就宕机了，会导致对应记录的丢失 可能会阻塞后续其他命令的执行，因为AOF记录日志实在redis主线程中执行的。 对比 RDB比AOF优秀的地方： RDB存储的是经过压缩后的二进制数据，保存某个时间点的内存快照，文件很小，适合做数据的备份、灾难恢复。AOF文件存储的是每一次的写命令，类似于MySQL的Binlog，通常比RDB文件大得多，而AOF文件过大时，会进行AOF文件重写，新的AOF比原有的AOF更小，但所保存的数据库状态一样。在Redis7.0之前，如果在重写期间有写入命令，AOF可能会使用大量内存，重写期间到达的写入命令都会写入磁盘两次。 使用RDB进行数据恢复，直接解析还原数据即可，不需要一条一条地执行命令，速度很快。而AOF需要依次执行每个命令，速度比较慢，也就是说，恢复大数据的时候，rdb比aof快。 AOF比RDB优秀的地方： RDB安全性不如AOF，没有办法实时或秒级持久化数据。生成RDB文件的过程是比较繁重的，虽然bgsave命令调用后台线程进行RBD文件的写入，但会对CPU资源产生影响，严重的话甚至会使Redis服务宕机。而AOF仅仅是追加命令到文件，操作轻量，且支持秒级持久化。RDB在两次备份之间，可能会有数据备份的遗漏。 RDB保存的是压缩后的二进制数据，且在Redis版本的迭代中，老版本的redis并不一定支持新版本的RDB文件。 AOF易于理解和解析。 综上： 对安全性要求不高，可以用RDB 不建议单独使用AOF，因为时不时地创建一个RDB快照可以进行数据库备份，更快地重启以及解决AOF引擎错误。 如果对安全性要求高，建议使用混合持久化。 校验机制 AOF校验是Redis在启动时对aof文件进行检查，检查其完整性、内容是否有损坏、数据是否有丢失。采用校验和来进行校验，采用的是CRC64算法，将校验和放在AOF文件的末尾，Redis启动的时候会计算文件的校验和，然后和文件末尾的校验和进行比对，若不一致，redis则提供错误信息。相应的，rdb也有类似的校验机制。 数据过期删除策略 **惰性删除：**只会在取出/查询 key 的时候才对数据进行过期检查。这种方式对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 **定期删除：**周期性地随机从设置了过期时间的 key 中抽查一批，然后逐个检查这些 key 是否过期，过期就删除 key。相比于惰性删除，定期删除对内存更友好，对 CPU 不太友好。 **延迟队列：**把设置过期时间的 key 放到一个延迟队列里，到期之后就删除 key。这种方式可以保证每个过期 key 都能被删除，但维护延迟队列太麻烦，队列本身也要占用资源。 **定时删除：**每个设置了过期时间的 key 都会在设置的时间到达时立即被删除。这种方法可以确保内存中不会有过期的键，但是它对 CPU 的压力最大，因为它需要为每个键都设置一个定时器。 Redis 采用的是定期删除+惰性/懒汉式删除结合的策略。 Redis 的定期删除过程是随机的（周期性地随机从设置了过期时间的 key 中抽查一批），所以并不保证所有过期键都会被立即删除。这也就解释了为什么有的 key 过期了，并没有被删除。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 控制删除操作的时长：执行时间已经超过了阈值，那么就中断这一次定期删除循环，以避免使用过多的 CPU 时间。 控制删除操作的频率：如果这一批过期的 key 比例超过一个比例，就会重复执行此删除流程，以更积极地清理过期 key。相应地，如果过期的 key 比例低于这个比例，就会中断这一次定期删除循环，避免做过多的工作而获得很少的内存回收。 数据淘汰策略 范围：allkeys/volatile 策略：lru/lfu/random 外加：volatile-ttl、no-eviction 平时用的比较多的是 allkeys-lru 分布式锁 分布式锁应具备的条件： 互斥：任意一个时刻，锁只能被一个线程持有。 高可用：锁服务是高可用的，当一个锁服务出现问题，能够自动切换到另外一个锁服务。并且，即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。这一般是通过超时机制实现的。 可重入：一个节点获取了锁之后，还可以再次获取锁。除了上面这三个基本条件之外，一个好的分布式锁还需要满足下面这些条件： 高性能：获取和释放锁的操作应该快速完成，并且不应该对整个系统的性能造成过大影响。 非阻塞：如果获取不到锁，不能无限期等待，避免对系统正常运行造成影响。 分布式锁的实现方案 Redis SETNX、Redisson ZooKeeper ZooKeeper 分布式锁是基于 临时顺序节点 和 Watcher（事件监听器） 实现的。 临时节点的生命周期是与客户端会话（session）绑定的，会话消失则节点消失，临时节点只能做叶子节点 获取锁： 首先我们要有一个持久节点/locks，客户端获取锁就是在locks下创建临时顺序节点。 假设客户端 1 创建了/locks/lock1节点，创建成功之后，会判断 lock1是否是 /locks 下最小的子节点。 如果 lock1是最小的子节点，则获取锁成功。否则，获取锁失败。 如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端 1 并不会不停地循环去尝试加锁，而是在前一个节点比如/locks/lock0上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了。 释放锁 成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。 成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。 前面说的事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。 注：lock节点是持久节点 两个方案如何选择？ 如果对性能要求比较高的话，建议使用 Redis 实现分布式锁（优先选择 Redisson 提供的现成的分布式锁，而不是自己实现） 如果对可靠性要求比较高的话，建议使用 ZooKeeper 实现分布式锁（推荐基于 Curator 框架实现）。不过，现在很多项目都不会用到 ZooKeeper，如果单纯是因为分布式锁而引入 ZooKeeper 的话，那是不太可取的，不建议这样做，为了一个小小的功能增加了系统的复杂度。 如何实现可重入？ 可重入是指，在一个线程中，可以多次获取同一把锁，比如一个线程正在执行一个带锁的方法，该方法调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法，这就是可重入，无需重新获得锁对象。 可重入锁的核心思想是，线程在请求锁的时候判断是否为自己的锁，如果是，则不用重新获取了。此外，还要为每个锁关联一个可重入计数器和占有它的线程，当计数器的值&gt;0时，则锁被占有，需要判断占有锁的线程和请求锁的线程是否是同一个。 如何解决集群下分布式锁的可靠性？ redis 红锁 zookeeper 接口的幂等性 ⭐TODO 在分布式系统中，幂等(idempotency)是对请求操作结果的一个描述，这个描述就是不论执行多少次相同的请求，产生的效果和返回的结果都和发出单个请求是一样的。 假如咱们的前后端没有保证接口幂等性，我作为用户在秒杀商品的时候，我同时点击了多次秒杀商品按钮，后端处理了多次相同的订单请求，结果导致一个人秒杀了多个商品。这个肯定是不能出现的，属于非常严重的 bug 了！ 前端保证幂等性的话比较简单，一般通过当用户提交请求后将按钮致灰来做到。后端保证幂等性就稍微麻烦一点，方法也是有很多种，比如： 同步锁； 分布式锁； 业务字段的唯一索性约束，防止重复数据产生。 … Redis性能优化 使用批量操作减少网络传输 一条redis命令的执行步骤可以分为四步：发送命令，命令排队，执行命令，返回结果。 其中第一步和第四步耗时之和称为Round Trip Time（RTT , 往返时间），就是数据在网络上传输的时间。 使用批量操作可以减少网络传输次数，进而减少网络开销，大幅减少RTT。 批量操作的方法： 原生批量操作命令： MGET(获取一个或多个指定 key 的值)、MSET(设置一个或多个指定 key 的值)、 HMGET(获取指定哈希表中一个或者多个指定字段的值)、HMSET(同时将一个或多个 field-value 对设置到指定哈希表中)、 SADD（向指定集合添加一个或多个元素） 但是，在 Redis 官方提供的分片集群解决方案 Redis Cluster 下，使用这些原生操作无法保证所有的 Key 都在同一个 hash slot 哈希槽上，所以这些原生批量操作还是有可能进行多次网络传输，不过相对于非批量操作，还是可以减少网络传输次数。 pipline 对于不支持批量操作的命令，我们可以利用 pipeline（流水线) 将一批 Redis 命令封装成一组，这些 Redis 命令会被一次性提交到 Redis 服务器，只需要一次网络传输。不过，需要注意控制一次批量操作的 元素个数(例如 500 以内，实际也和元素字节数有关)，避免网络传输的数据量过大。与MGET、MSET等原生批量操作命令一样，pipeline 同样在 Redis Cluster 上使用会存在一些小问题。原因类似，无法保证所有的 key 都在同一个 hash slot（哈希槽）上。如果想要使用的话，客户端需要自己维护 key 与 slot 的关系。 原生批量操作命令和 pipeline 的是有区别的，使用的时候需要注意： 原生批量操作命令是原子操作，pipeline 是非原子操作。 pipeline 可以打包不同的命令，原生批量操作命令不可以。 原生批量操作命令是 Redis 服务端支持实现的，而 pipeline 需要服务端和客户端的共同实现。 Lua脚本 大量key集中过期影响性能 对于过期的Key，redis采用的是定时删除+惰性删除模式。定时删除任务执行时，若突然遇到大量过期Key，客户端必须等待定时清理任务线程执行完成，这个定时清理任务线程实在redis主线程执行的，这就导致客户端的请求无法被即使处理。 解决方案： 给key设置随机过期时间 开启lazy-free（惰性删除/延迟释放），让redis采用异步的方式删除过期key，该操作会交给子线程执行，从而避免阻塞主线程。 Bigkey影响性能 redis中，如果一个key的value过大，则被视为bigkey。 String 类型的value超过1MB，复合类型（List, Hash, Set, ZSet）的 value 元素超过5000个 bigkey会对redis性能造成影响： bigkey 会占用更多的内存和网络带宽，会造成阻塞问题： 客户端超时阻塞：redis执行命令是单线程进行，在操作bigkey时会比较耗时，那么就会阻塞redis，从客户端的角度来看，就是很久没有得到服务端的响应。 网络阻塞：每次获取bigkey的网络流量比较大，若一个bigkey大小为1MB，qps = 1000，那么每秒就会产生1000MB的流量，对于普通千兆网卡的服务器来说压力很大。 工作线程阻塞：如果使用del删除bigkey时，会阻塞工作线程，会导致排队的命令无法执行。 bigkey是如何产生的？ 程序设计不当，比如直接使用 String 类型存储较大的文件对应的二进制数据。 对于业务的数据规模考虑不周到，比如使用集合类型的时候没有考虑到数据量的快速增长。 未及时清理垃圾数据，比如哈希中冗余了大量的无用键值对。 如何处理bigkey? 分割 bigkey：将一个 bigkey 分割为多个小 key。例如，将一个含有上万字段数量的 Hash 按照一定策略（比如二次哈希）拆分为多个 Hash。 手动清理：Redis 4.0+ 可以使用 UNLINK 命令来异步删除一个或多个指定的 key。Redis 4.0 以下可以考虑使用 SCAN 命令结合 DEL 命令来分批次删除。 采用合适的数据结构：例如，文件二进制数据不使用 String 保存、使用 HyperLogLog 统计页面 UV、Bitmap 保存状态信息（0/1）。 开启 lazy-free（惰性删除/延迟释放） ：lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。 Hotkey影响性能 如果一个key的访问次数比较多且明显多于其他key，则该key可以看作hotkey； 比如Redis实例每秒处理的请求为5000，其中2000是处理某个key，则该key可以视为hotkey Hotkey的危害： 处理 hotkey 会占用大量的 CPU 和带宽，可能会影响 Redis 实例对其他请求的正常处理。此外，如果突然访问 hotkey 的请求超出了 Redis 的处理能力，Redis 就会直接宕机。这种情况下，大量请求将落到后面的数据库上，可能会导致数据库崩溃。 如何发现hotkey? 可以使用–hotkeys参数来查找，不过前提是，redis的maxmemory-policy设置为LFU，redis 的maxmemory-policy有volatile-lfu和allkeys-lfu 使用MONITOR命令，该命令可以实时查看redis的所有操作，可以用于临时监控redis实例的操作情况，包括读、写，但对性能形象太大，禁止长期使用。 如何解决hotkey? 读写分离：主节点处理写请求，从节点处理读请求。 使用 Redis Cluster：将热点数据分散存储在多个 Redis 节点上。 二级缓存：hotkey 采用二级缓存的方式进行处理，将 hotkey 存放一份到 JVM 本地内存中（可以用 Caffeine）。 慢查询命令 redis命令执行步骤为：发送命令、命令排队、命令执行、返回结果 慢查询针对的是命令执行这一步。 在redis.conf中，使用slow-log-slower-than参数设置耗时命令的阈值，并使用slowlog-max-len设置记录耗时命令的最大条数。 使用SLOWLOG GET获取慢查询日志 Redis常见阻塞原因 O(n) 命令 Redis 中的大部分命令都是 O(1)时间复杂度，但也有少部分 O(n) 时间复杂度的命令，例如： KEYS *：会返回所有符合规则的 key。 HGETALL：会返回一个 Hash 中所有的键值对 LRANGE：会返回 List 中指定范围内的元素。 SMEMBERS：返回 Set 中的所有元素。 SINTER/SUNION/SDIFF：计算多个 Set 的交集/并集/差集 RDB创建 Redis 提供了两个命令来生成 RDB 快照文件： save : 同步保存操作，会阻塞 Redis 主线程； bgsave : fork 出一个子进程，子进程执行，不会阻塞 Redis 主线程，默认选项。 默认情况下，Redis 默认配置会使用 bgsave 命令。如果手动使用 save 命令生成 RDB 快照文件的话，就会阻塞主线程。 AOF 日志记录阻塞：我们知道，AOF 是在当前命令执行完之后在进行记录的，这不会阻塞当前命令的执行，但可能阻塞后续命令的执行。 AOF刷盘阻塞：当后台线程（ aof_fsync 线程）调用 fsync 函数同步 AOF 文件时，需要等待，直到写入完成。当磁盘压力太大的时候，会导致 fsync 操作发生阻塞，主线程调用 write 函数时也会被阻塞。fsync 完成后，主线程执行 write 才能成功返回。 AOF重写阻塞： BigKey CPU竞争 Redis 是典型的 CPU 密集型应用，不建议和其他多核 CPU 密集型服务部署在一起。当其他进程过度消耗 CPU 时，将严重影响 Redis 的吞吐量。 可以通过redis-cli --stat获取当前 Redis 使用情况。通过top命令获取进程对 CPU 的利用率等信息 通过info commandstats统计信息分析出命令不合理开销时间，查看是否是因为高算法复杂度或者过度的内存优化问题 网络问题 Swap Redis内存碎片 redis产生内存碎片的原因： Redis存储数据时向操作系统申请的内存空间大于实际上存储数据所需的空间 Redis 使用 zmalloc 方法(Redis 自己实现的内存分配方法)进行内存分配的时候，除了要分配 size 大小的内存之外，还会多分配 PREFIX_SIZE 大小的内存。 另外，Redis 可以使用多种内存分配器来分配内存（ libc、jemalloc、tcmalloc），默认使用 jemalloc，而 jemalloc 按照一系列固定的大小（8 字节、16 字节、32 字节……）来分配内存的。 当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间，就比如说程序需要申请 17 字节的内存，jemalloc 会直接给它分配 32 字节的内存，这样会导致有 15 字节内存的浪费。不过，jemalloc 专门针对内存碎片问题做了优化，一般不会存在过度碎片化的问题。 频繁修改 Redis 中的数据也会产生内存碎片。 当 Redis 中的某个数据删除时，Redis 通常不会轻易释放内存给操作系统。 Redis Sentinel 主从模式：高并发 哨兵模式：高可用 集群模式：哨兵模式上进一步提高并发量 普通的主从复制方案下，一旦 master 宕机，我们需要从 slave 中手动选择一个新的 master，同时需要修改应用方的主节点地址，还需要命令所有从节点去复制新的主节点，整个过程需要人工干预。人工干预大大增加了问题的处理时间以及出错的可能性。 我们可以借助 Redis 官方的 Sentinel（哨兵）方案来帮助我们解决这个痛点，实现自动化地故障切换。 什么是 Sentinel Sentinel（哨兵） 只是 Redis 的一种运行模式 ，不提供读写服务，默认运行在 26379 端口上， Redis 在 Sentinel 这种特殊的运行模式下，使用专门的命令表，也就是说普通模式运行下的 Redis 命令将无法使用。 Redis Sentinel 实现 Redis 集群高可用，只是在主从复制实现集群的基础下，多了一个 Sentinel 角色来帮助我们监控 Redis 节点的运行状态并自动实现故障转移。 Sentinel 有什么用 **监控：**监控所有 redis 节点（包括 sentinel 节点自身）的状态是否正常。 **故障转移：**如果一个 master 出现故障，Sentinel 会帮助我们实现故障转移，自动将某一台 slave 升级为 master，确保整个 Redis 系统的可用性。 **通知 ：**通知 slave 新的 master 连接信息，让它们执行 replicaof 成为新的 master 的 slave。 **配置提供 ：**客户端连接 sentinel 请求 master 的地址，如果发生故障转移，sentinel 会通知新的 master 链接信息给客户端。 Redis Sentinel 本身设计的就是一个分布式系统，建议多个 sentinel 节点协作运行。这样做的好处是： 多个 sentinel 节点通过投票的方式来确定 sentinel 节点是否真的不可用，避免误判（比如网络问题可能会导致误判）。 Sentinel 自身就是高可用。 如果想要实现高可用，建议将哨兵 Sentinel 配置成单数且大于等于 3 台。 Sentinel 如何检测节点下线 Redis Sentinel 中有两个下线（Down）的概念： **主观下线(SDOWN) ：**sentinel 节点认为某个 Redis 节点已经下线了（主观下线），但还不是很确定，需要其他 sentinel 节点的投票。 **客观下线(ODOWN) ：**法定数量（通常为过半）的 sentinel 节点认定某个 Redis 节点已经下线（客观下线），那它就算是真的下线了。 也就是说，主观下线 当前的 sentinel 自己认为节点宕机，客观下线是 sentinel 整体达成一致认为节点宕机。 每个 sentinel 节点以每秒钟一次的频率向整个集群中的 master、slave 以及其他 sentinel 节点发送一个 PING 命令。 如果对应的节点超过规定的时间（down-after-millisenconds）没有进行有效回复的话，就会被其认定为是 主观下线(SDOWN) 如果被认定为主观下线的是 slave 的话， sentinel 不会做什么事情，因为 slave 下线对 Redis 集群的影响不大，Redis 集群对外正常提供服务。但如果是 master 被认定为主观下线就不一样了，sentinel 整体还要对其进行进一步核实，确保 master 是真的下线了。 所有 sentinel 节点要以每秒一次的频率确认 master 的确下线了，当法定数量（通常为过半）的 sentinel 节点认定 master 已经下线， master 才被判定为 客观下线(ODOWN) 。这样做的目的是为了防止误判，毕竟故障转移的开销还是比较大的，这也是为什么 Redis 官方推荐部署多个 sentinel 节点（哨兵集群）。 如何选出新Master slave 必须是在线状态才能参加新的 master 的选举，筛选出所有在线的 slave 之后，通过下面 3 个维度进行最后的筛选（优先级依次降低）： **slave 优先级 ：**可以通过 slave-priority 手动设置 slave 的优先级，优先级越高得分越高，优先级最高的直接成为新的 master。如果没有优先级最高的，再判断复制进度。 **复制进度 ：**Sentinel 总是希望选择出数据最完整（与旧 master 数据最接近）也就是复制进度最快的 slave 被提升为新的 master，复制进度越快得分也就越高。 **runid(运行 id) ：**通常经过前面两轮筛选已经成果选出来了新的 master，万一真有多个 slave 的优先级和复制进度一样的话，那就 runid 小的成为新的 master，每个 redis 节点启动时都有一个 40 字节随机字符串作为运行 id。 Sentinel 是否可以防止脑裂 什么是脑裂？ 简单来说就是主库发生了假故障。如果当前主库突然出现暂时性 “失联”，而并不是真的发生了故障，此时监听的哨兵会自动启动主从切换机制。当这个原始的主库从假故障中恢复后，又开始处理请求，但是哨兵已经选出了新的主库，这样一来，旧的主库和新主库就会同时存在，这就是脑裂现象。 脑裂有什么影响？产生脑裂后，原有的客户端还会在原来的 master 上继续写入数据，新的 master 无法同步这些数据到自身，新的数据也无法同步到老节点，造成Redis 数据的不一致。当网络分区解决后，sentinel 会将老 master 降级为 slave ，此时被降级为 slave 的节点要从 master 中同步数据，他原有的新增数据就会丢失。 如何解决脑裂问题？ Redis 中有两个关键的配置项可以解决这个问题，分别是 min-slaves-to-write（最小从服务器数） 和 min-slaves-max-lag（从连接的最大延迟时间）。 min-slaves-to-write 是指主库最少得有 N 个健康的从库存活才能执行写命令。这个配置虽然不能保证 N 个从库都一定能接收到主库的写操作，但是能避免当没有足够健康的从库时，主库拒绝写入，以此来避免数据的丢失 ，如果设置为 0 则表示关闭该功能。 min-slaves-max-lag 这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟。用于配置 master 多长时间（秒）无法得到从节点的响应，就认为这个节点失联。我们这里配置的是 10 秒，也就是说 master 10 秒都得不到一个从节点的响应，就会认为这个从节点失联，停止接受新的写入命令请求。 配置了这两个参数后，如果发生脑裂，原先的master 节点就会拒绝写入操作，会在新的master 节点进行数据写入，从缺避免数据丢失。 Redis Cluster 为什么需要 Redis Cluster？解决了什么问题？有什么优势？ Redis Cluster 是如何分片的？ 为什么 Redis Cluster 的哈希槽是 16384 个? 如何确定给定 key 的应该分布到哪个哈希槽中？ Redis Cluster 支持重新分配哈希槽吗？ Redis Cluster 扩容缩容期间可以提供服务吗？ Redis Cluster 中的节点是怎么进行通信的？ Redis Cluster 主要是为了提高写并发。 为了保证高可用，Redis Cluster 至少需要 3 个 master 以及 3 个 slave，也就是说每个 master 必须有 1 个 slave。master 和 slave 之间做主从复制，slave 会实时同步 master 上的数据。 不同于普通的 Redis 主从架构，这里的 slave 不对外提供读服务，主要用来保障 master 的高可用，当 master 出现故障的时候替代它。 Redis Cluster 是去中心化的（各个节点基于 Gossip 进行通信），任何一个 master 出现故障，其它的 master 节点不受影响，因为 key 找的是哈希槽而不是 Redis 节点。不过，Redis Cluster 至少要保证宕机的 master 有一个 slave 可用。 如果宕机的 master 无 slave 的话，为了保障集群的完整性，保证所有的哈希槽都指派给了可用的 master ，整个集群将不可用。这种情况下，还是想让集群保持可用的话，可以将cluster-require-full-coverage 这个参数设置成 no，cluster-require-full-coverage 表示需要 16384 个 slot 都正常被分配的时候 Redis Cluster 才可以对外提供服务。 有了 Redis Cluster 之后，不需要专门部署 Sentinel 集群服务了。Redis Cluster 相当于是内置了 Sentinel 机制，Redis Cluster 内部的各个 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可以自动切换。 Redis Cluster 如何分片 Redis Cluster 并没有使用一致性哈希，采用的是 哈希槽分区 ，每一个键值对都属于一个 hash slot（哈希槽） 。 Redis Cluster 通常有 16384 个哈希槽 ，要计算给定 key 应该分布到哪个哈希槽中，我们只需要先对每个 key 计算 CRC-16（XMODEM） 校验码，然后再对这个校验码对 16384(哈希槽的总数) 取模，得到的值即是 key 对应的哈希槽。 创建并初始化 Redis Cluster 的时候，Redis 会自动平均分配这 16384 个哈希槽到各个节点，不需要我们手动分配。如果你想自己手动调整的话，Redis Cluster 也内置了相关的命令比如 ADDSLOTS、ADDSLOTSRANGE 客户端连接 Redis Cluster 中任意一个 master 节点即可访问 Redis Cluster 的数据，当客户端发送命令请求的时候，需要先根据 key 通过上面的计算公示找到的对应的哈希槽，然后再查询哈希槽和节点的映射关系，即可找到目标节点。 如果哈希槽确实是当前节点负责，那就直接响应客户端的请求返回结果，如果不由当前节点负责，就会返回 -MOVED 重定向错误，告知客户端当前哈希槽是由哪个节点负责，客户端向目标节点发送请求并更新缓存的哈希槽分配信息。 Redis Cluster 哈希槽分区机制的优点：解耦了数据和节点之间的关系，提升了集群的横向扩展性和容错性。 Redis Cluster 扩容缩容期间可以提供服务吗？ 为了保证 Redis Cluster 在扩容和缩容期间依然能够对外正常提供服务，Redis Cluster 提供了重定向机制，两种不同的类型： ASK 重定向 MOVED 重定向 从客户端的角度来看，ASK 重定向是下面这样的： 客户端发送请求命令，如果请求的 key 对应的哈希槽还在当前节点的话，就直接响应客户端的请求。 如果客户端请求的 key 对应的哈希槽当前正在迁移至新的节点，就会返回 -ASK 重定向错误，告知客户端要将请求发送到哈希槽被迁移到的目标节点。 客户端收到 -ASK 重定向错误后，将会临时（一次性）重定向，自动向目标节点发送一条 ASKING 命令。也就是说，接收到 ASKING 命令的节点会强制执行一次请求，下次再来需要重新提前发送 ASKING 命令。 客户端发送真正的请求命令。 ASK 重定向并不会同步更新客户端缓存的哈希槽分配信息，也就是说，客户端对正在迁移的相同哈希槽的请求依然会发送到原节点而不是目标节点。 如果客户端请求的 key 对应的哈希槽应该迁移完成的话，就会返回 -MOVED 重定向错误，告知客户端当前哈希槽是由哪个节点负责，客户端向目标节点发送请求并更新缓存的哈希槽分配信息。 Redis实现延时任务 过期事件监听 在 pub/sub 模式下，生产者需要指定消息发送到哪个 channel 中，而消费者则订阅对应的 channel 以获取消息。 Redis 中有很多默认的 channel，这些 channel 是由 Redis 本身向它们发送消息的，而不是我们自己编写的代码。其中，__keyevent@0__:expired 就是一个默认的 channel，负责监听 key 的过期事件。也就是说，当一个 key 过期之后，Redis 会发布一个 key 过期的事件到__keyevent@&lt;db&gt;__:expired这个 channel 中。我们只需要监听这个 channel，就可以拿到过期的 key 的消息，进而实现了延时任务功能。这个功能被 Redis 官方称为 keyspace notifications ，作用是实时监控实时监控 Redis 键和值的变化。 Redis过期事件监听的缺点： 时效性差：过期事件是在Redis删除key时发布的，但 key 并不是一过期就直接删除的。 丢消息：pub/sub不支持持久化，并且没有订阅者时，消息会直接丢弃，不会存储在channel中 多服务实例下消息重复消费：Redis 的 pub/sub 模式目前只有广播模式，这意味着当生产者向特定频道发布一条消息时，所有订阅相关频道的消费者都能够收到该消息。这个时候，我们需要注意多个服务实例重复处理消息的问题，这会增加代码开发量和维护难度。 延迟队列 Redisson 使用 zrangebyscore 命令扫描 SortedSet 中过期的元素，然后将这些过期元素从 SortedSet 中移除，并将它们加入到就绪消息列表中。就绪消息列表是一个阻塞队列，有消息进入就会被监听到。这样做可以避免对整个 SortedSet 进行轮询，提高了执行效率。 相对于过期事件监听，延迟队列有以下优势： 减少了丢消息的可能：因为DelayedQueue中的消息会被持久化 消息不存在重复消费：每个客户端都是从同一个目标队列获取任务的，不存在重复消费问题。","tags":["Redis","KV数据库"],"categories":["Redis","数据库"]},{"title":"halo安装部署","path":"/2024/06/09/halo安装部署/","content":"安装Docker 使用终端命令安装Docker 更新yum yum update 卸载旧版本： 列出安装过的 Docker 包： yum list installed | grep docker 旧版名称是 Docker，最新社区版 docker-engine， 目前已改名为docker-ce yum -y remove docker docker-common docker-selinux docker-engine 设置yum源： 本文以 yum 安装为例子进行安装安装 yum-utils，使用 yum-config-manager 工具设置 yum 源，后面两个是 devicemapper 驱动依赖 yum install -y yum-utils device-mapper-persistent-data lvm2 使用阿里源访问 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装Docker yum install docker-ce 安装成功 启动Docker,并加入开启自启 systemctl start docker 启动 systemctl restart docker 重启systemctl enable docker 开机启动 执行 docker version 查看 Docker 版本号。 并且最新的docker 已经集成了 docker compose 安装halo 创建 halo 目录 mkdir ~/halo &amp;&amp; cd ~/halo 创建 docker-compose.yaml 此处我们使用 mysql 作为数据库 docker-compose.yaml 的内容如下 version: &quot;3&quot;services: halo: image: halohub/halo:2.16 restart: on-failure:3 depends_on: halodb: condition: service_healthy networks: halo_network: volumes: - ./halo2:/root/.halo2 ports: - &quot;8090:8090&quot; healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:8090/actuator/health/readiness&quot;] interval: 30s timeout: 5s retries: 5 start_period: 30s command: - --spring.r2dbc.url=r2dbc:pool:mysql://halodb:3306/halo - --spring.r2dbc.username=root # MySQL 的密码，请保证与下方 MYSQL_ROOT_PASSWORD 的变量值一致。 - --spring.r2dbc.password=o#DwN&amp;JSa56 - --spring.sql.init.platform=mysql # 外部访问地址，请根据实际需要修改 - --halo.external-url=http://localhost:8090/ halodb: image: mysql:8.1.0 restart: on-failure:3 networks: halo_network: command: - --default-authentication-plugin=caching_sha2_password - --character-set-server=utf8mb4 - --collation-server=utf8mb4_general_ci - --explicit_defaults_for_timestamp=true volumes: - ./mysql:/var/lib/mysql - ./mysqlBackup:/data/mysqlBackup healthcheck: test: [&quot;CMD&quot;, &quot;mysqladmin&quot;, &quot;ping&quot;, &quot;-h&quot;, &quot;127.0.0.1&quot;, &quot;--silent&quot;] interval: 3s retries: 5 start_period: 30s environment: # 请修改此密码，并对应修改上方 Halo 服务的 SPRING_R2DBC_PASSWORD 变量值 - MYSQL_ROOT_PASSWORD=o#DwN&amp;JSa56 - MYSQL_DATABASE=halonetworks: halo_network: 启动 Halo docker compose up -d docker compose logs -f 用浏览器访问 /console 即可进入 Halo 管理页面，首次启动会进入初始化页面。 后续如果需要修改 halo 的配置，直接更新 docker-compose.yaml 后重新启动容器即可 如果遇到DockerHub被DNS污染的情况，会拉取不到镜像，那就取ghcr.io上拉取halo的镜像 nginx 反向代理 安装nginx sudo yum install nginx #安装sudo systemctl start nginx #启动sudo systemctl enable nginx #开机自启 配置反向代理 upstream halo &#123; server 127.0.0.1:8090;&#125;server &#123; listen 80; listen [::]:80; server_name www.yourdomain.com; client_max_body_size 1024m; location / &#123; proxy_pass http://halo; proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 另外，halo2是用java写的，相比于带后端的其他博客系统，比如typecho，wordpress来说，还是比较吃内存的，一个mysql容器，一个halo容器，一个nginx，占用了1G多内存，没办法，java应用是这样的","tags":["博客","运维"],"categories":["博客","运维"]},{"title":"JUC知识点","path":"/2024/06/08/JUC知识点/","content":"在JAVA中，当我们启动main函数时，实际上会启动一个JVM进程，main函数所在的线程就是这个进程的主线程。 并发编程的三个重要特性：原子性、可见性、有序性 原子性：使用sychronized、各种lock保证原子性 可见性：当一个线程对共享变量修改后，另外的线程都是可以立即看到修改后的最新值。使用sychronized、volatile、lock实现可见性。 有序性：指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致，所以在多线程下，指令重排序可能会导致一些问题，我们可以使用volatile关键字来禁止指令重排序。 多线程 进程和线程的区别 并发和并行 创建线程的方式 public class test &#123; public static void main(String[] args) &#123; //匿名内部类，在Thread()中传入实现了Rannable接口的实例 new Thread(()-&gt;&#123; System.out.println(&quot;hello&quot;); &#125;).start(); &#125;&#125; Runnable和Callable有什么区别 run() 和 start()的区别 线程的状态及转化 如何控制线程的运行顺序 join() notify()和notifyall() sleep()和wait() ⭐ 停止线程 //使用标志退出public class MyInterrupt1 extends Thread &#123; volatile boolean flag = false ; // 线程执行的退出标记 @Override public void run() &#123; while(!flag) &#123; System.out.println(&quot;MyThread...run...&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; // 创建MyThread对象 MyInterrupt1 t1 = new MyInterrupt1() ; t1.start(); // 主线程休眠6秒 Thread.sleep(6000); // 更改标记为true t1.flag = true ; &#125;&#125; //使用interrupt方法中断线程package com.itheima.basic;public class MyInterrupt3 &#123; public static void main(String[] args) throws InterruptedException &#123; //1.打断阻塞的线程 /*Thread t1 = new Thread(()-&gt;&#123; System.out.println(&quot;t1 正在运行...&quot;); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(500); t1.interrupt(); System.out.println(t1.isInterrupted());*/ //2.打断正常的线程 Thread t2 = new Thread(()-&gt;&#123; while(true) &#123; Thread current = Thread.currentThread(); boolean interrupted = current.isInterrupted(); if(interrupted) &#123; System.out.println(&quot;打断状态：&quot;+interrupted); break; &#125; &#125; &#125;, &quot;t2&quot;); t2.start(); Thread.sleep(500);// t2.interrupt(); &#125;&#125; synchronized底层原理 基本使用 注意：sychronized只能满足单个jvm下的锁，多个jvm下需要分布式锁，如同一个服务，但是做了集群，他们就位于不同的jvm中 Synchronized【对象锁】采用互斥的方式让同一时刻至多只有一个线程能持有【对象锁】，其它线程再想获取这个【对象锁】时就会阻塞住 使用方式： 修饰实例方法（锁当前对象实例） 给当前对象实例加锁，进入同步代码前要先获得当前对象的实例锁 修饰静态方法（锁当前类） 给当前类加锁，会作用于类的所有实例对象，进入同步代码前要先获得当前class的锁，因为静态成员变量不属于任何一个实例，而是属于整个类，被类的所用实例共享。 静态 synchronized 方法和非静态 synchronized 方法之间的调用互斥么？不互斥！如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块（锁指定对象/类） 对括号里的 类/对象 加锁 synchronized(object) 表示进入同步代码库前要获得 给定对象的锁。 synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁 构造方法能用 synchronized 修饰吗？ 不能，但构造方法内可以使用 synchronized 代码块另外，构造方法本身是线程安全的，但如果在构造方法中涉及到共享资源的操作，就需要采取适当的同步措施来保证整个构造过程的线程安全 底层原理 synchronized 关键字底层原理属于 JVM 层面的东西。 Monitor 被翻译为监视器，由jvm提供，c++语言实现 修饰同步代码块的情况 public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println(&quot;synchronized 代码块&quot;); &#125; &#125;&#125; 以下为上述代码字节码文件的一部分，可以看到，包含两个monitorexit，这是为了保证同步代码块正常执行以及出现异常时，同步锁可以正常释放。 当执行monitorenter指令时，线程会试图获取锁，也就是获取对象监视器monitor的特有权。 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 ObjectMonitor对象。 另外，wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。 具体的流程： 代码进入synchorized代码块，先让lock（对象锁）关联monitor，然后判断Owner是否有线程持有 如果没有线程持有，则让当前线程持有，表示该线程获取锁成功 如果有线程持有，则让当前线程进入entryList进行阻塞，如果Owner持有的线程已经释放了锁，在EntryList中的线程去竞争锁的持有权==（非公平）== 如果代码块中调用了wait()方法，则会进去WaitSet中进行等待 参考回答： Synchronized【对象锁】采用互斥的方式让同一时刻至多只有一个线程能持有【对象锁】 它的底层由monitor实现的，monitor是jvm级别的对象（ C++实现），线程获得锁需要使用对象（锁）关联monitor 在monitor内部有三个属性，分别是owner、entrylist、waitset 其中owner是关联的获得锁的线程，并且只能关联一个线程；entrylist关联的是处于阻塞状态的线程；waitset关联的是处于Waiting状态的线程 修饰方法的情况 public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println(&quot;synchronized 方法&quot;); &#125;&#125; synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 如果是实例方法，JVM 会尝试获取实例对象的锁。如果是静态方法，JVM 会尝试获取当前 class 的锁 总结 synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。 synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。 锁升级 首先，锁的状态有4中：无锁状态、偏向锁、轻量级锁、重量级锁。这些都记录在锁对象的Mark Word中。随着锁竞争的激烈程度上升，锁会升级，但不会降级。 偏向锁 偏向于第一个请求锁的线程。 如果在运行过程中，同步锁只有一个线程请求，不存在锁竞争的情况，则会给该线程加一个偏向锁（将线程id记录在锁对象mark word中），当该线程下次执行同步代码块时，会判断当前持有锁的线程是否是自己。如果自始至终都没有锁竞争，那么偏向锁不会有额外的开销，效率很高。 如果运行过程中发生了锁竞争，则持有偏向锁的线程会被挂起，JVM会消除他的偏向锁，将锁升级为轻量级锁，撤销轻量级锁时会导致STW。 轻量级锁（自旋锁） 升级为轻量级锁后，竞争失败的线程会导致锁进入锁膨胀状态，会让竞争失败的线程自旋，自旋会导致忙等问题，当自旋达到一定次数后，轻量级锁升级为重量级锁。 自旋的好处是，减少了线程状态切换带来的开销，缺点是可能会占用CPU资源。 重量级锁 当后续线程尝试获取锁时，发现被占用的锁是重量级锁，则自己会直接进入阻塞状态。 volatile 保证线程间的可见性 如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取 其实是加了volatile后，每次读写这个变量，都要到共享内存中读取 禁止指令重排序 双重校验锁 实现对象单例（线程安全）： public class Singleton &#123; private volatile static Singleton uniqueInstance;\t//私有化构造函数，不能 new 对象 private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 **volatile 关键字能保证变量的可见性，但不能保证对变量的操作是原子性的。**通常，对一个变量的修改包括三部：读取变量值，修改变量值，将修改后的值保存 synchronized和volatile synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！ volatile 关键字是线程同步的轻量级实现，所以 volatile性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块 。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，和防止指令重排，而 synchronized 关键字解决的是多个线程之间访问资源的同步性 JMM(Java内存模型) JMM 定义了共享内存中多线程程序读写操作的行为规范。 什么是主内存？什么是本地内存？ 主内存：所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量，还是局部变量，类信息、常量、静态变量都是放在主内存中。为了获取更好的运行速度，虚拟机及硬件系统可能会让工作内存优先存储于寄存器和高速缓存中。 本地内存：每个线程都有一个私有的本地内存，本地内存存储了该线程以读 / 写==共享变量的副本==。**每个线程只能操作自己本地内存中的变量，无法直接访问其他线程的本地内存。如果线程间需要通信，必须通过主内存来进行。**本地内存是 JMM 抽象出来的一个概念，并不真实存在，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 JMM为共享变量提供了可见性的保障。 主内存存放共享变量，工作内存存放共享变量副本。 锁 悲观锁 悲观锁的基本思想是认为数据很可能会发生冲突，因此在访问数据之前先获取锁。在使用悲观锁的情况下，线程在访问共享资源之前会先尝试获取锁，如果获取不到锁就会进入阻塞状态，直到获取到锁为止。 悲观锁常常使用 synchronized 关键字或者是显式锁（如 ReentrantLock）来实现，它可以确保在同一时刻只有一个线程能够访问共享资源，从而保证数据的一致性。 AQS（公平的） AbstractQueueSynchronizer ,抽象队列同步器，是构建锁或其他同步组件的基础框架 AQS 就是一个抽象类，主要用来构建锁和同步器。 常见的实现类：ReentrantLock、Semaphore、CountDownLatch ReentrantLock 基于JDK实现的锁，与synchronized相比具有以下特点： 可中断 可重入（synchronized也可重入） 可设置公平锁 可设置超时时间 支持多个条件变量 可中断锁：获取锁的过程中可以被中断，不需要一直等到获取锁之后才能进行其他逻辑处理。ReentrantLock 就属于是可中断锁。 通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 不可中断锁：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 synchronized 就属于是不可中断锁。 可重入锁 也叫递归锁，指的是同一个线程可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果是不可重入锁的话，就会造成死锁。 底层原理 AQS+CAS synchronized 与 Lock的区别 语法层面： synchronized 是关键字，由 jvm 实现，用cpp实现 Lock 是接口，由 JDK 提供，用 java 语言实现 synchornized 退出同步块会自动释放锁，Lock 要手动 unlock 功能层面 都是悲观锁，具有基本的互斥、同步、重入功能 Lock 可打断、可公平、可设置超时时间、多条件变量 Lock 由适合不同场景的实现，如 ReentrantLock、ReentrantReadWriteLock 性能层面： 没有竞争时，synchronized 做了很多优化，如偏向锁、轻量级锁，性能不错 竞争激烈时，Lock 性能更好 Semaphore synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，而Semaphore(信号量)可以用来控制同时访问特定资源的线程数量 // 初始共享资源数量final Semaphore semaphore = new Semaphore(5);// 获取1个许可semaphore.acquire();// 释放1个许可semaphore.release(); Semaphore 通常用于那些资源有明确访问数量限制的场景比如限流（仅限于单机模式，实际项目中推荐使用 Redis +Lua 来做限流）。 原理 Semaphore 是共享锁的一种实现，它默认构造 AQS 的 state 值为 permits，你可以将 permits 的值理解为许可证的数量，只有拿到许可证的线程才能执行。 调用semaphore.acquire() ，线程尝试获取许可证，如果 state &gt;= 0 的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 state 的值 state=state-1。如果 state&lt;0 的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程。 死锁 死锁的四个必要条件 互斥条件 ：该资源任意一个时刻只由一个线程占用。 请求与保持条件 ：一个线程因请求资源而阻塞时，对已获得的资源保持不放。 **不剥夺条件 **: 线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 **循环等待条件 **: 若干线程之间形成一种头尾相接的循环等待资源关系。 public class DeadLockDemo &#123; private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) &#123; new Thread(() -&gt; &#123; synchronized (resource1) &#123; System.out.println(Thread.currentThread() + &quot;get resource1&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread() + &quot;waiting get resource2&quot;); synchronized (resource2) &#123; System.out.println(Thread.currentThread() + &quot;get resource2&quot;); &#125; &#125; &#125;, &quot;线程 1&quot;).start(); new Thread(() -&gt; &#123; synchronized (resource2) &#123; System.out.println(Thread.currentThread() + &quot;get resource2&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread() + &quot;waiting get resource1&quot;); synchronized (resource1) &#123; System.out.println(Thread.currentThread() + &quot;get resource1&quot;); &#125; &#125; &#125;, &quot;线程 2&quot;).start(); &#125;&#125; 诊断死锁 乐观锁 乐观锁的基本思想是认为数据在一般情况下不会发生冲突，因此在访问数据时不加锁，而是在更新数据时检查是否发生了冲突。如果发现冲突，会进行相应的处理（通常是回滚操作），然后重新尝试。 乐观锁的典型实现是版本号机制、CAS算法（是一个原子操作）。 在 Java 中java.util.concurrent.atomic包下面的原子变量类（比如AtomicInteger、LongAdder）就是使用了乐观锁的一种实现方式 CAS 实现的。 悲观锁乐观锁如何选择 悲观锁适用于并发写入较多的场景，能够确保数据的一致性；而乐观锁适用于并发读取较多、写入较少的场景，可以提高系统的并发性能。 高并发的场景下，乐观锁相比悲观锁来说，不存在锁竞争造成线程阻塞，也不会有死锁的问题，在性能上往往会更胜一筹。但是，如果冲突频繁发生（写占比非常多的情况），会频繁失败和重试（黑马点评，超卖问题的解决），这样同样会非常影响性能，导致 CPU 飙升。 悲观锁通常多用于写比较多的情况（多写场景，竞争激烈），这样可以避免频繁失败和重试影响性能，悲观锁的开销是固定的。不过，如果乐观锁解决了频繁失败和重试这个问题的话（比如LongAdder），也是可以考虑使用乐观锁的，要视实际情况而定。 乐观锁通常多用于写比较少的情况（多读场景，竞争较少），这样可以避免频繁加锁影响性能。不过，乐观锁主要针对的对象是单个共享变量（参考java.util.concurrent.atomic包下面的原子变量类） CAS（Compare And Swap) ​ CAS 的全称是 Compare And Swap（比较与交换） ，用于实现乐观锁，被广泛应用于各大框架中。CAS 的思想很简单，就是用一个预期值和要更新的变量值进行比较，两值相等才会进行更新。 CAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。 原子操作 即最小不可拆分的操作，也就是说操作一旦开始，就不能被打断，直到操作完成。 CAS 涉及到三个操作数： V：要更新的变量值(Var) E：预期值(Expected) N：拟写入的新值(New) 当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。 CAS存在的问题 ABA问题，利用版本号机制可以解决 自旋问题，循环时间长开销大：当操作不成功的时候会一直自旋，直到操作成功，会占用过多CPU资源 只能保证一个共享变量的原子操作：只对单个共享变量有效，当涉及到多个共享变量CAS无效 多线程的执行安全 要确保三大特性：原子性、可见性、有序性 原子性 一个线程在CPU中操作不可暂停，也不可中断，要不执行完成，要不不执行 实现方案： 加锁 synchronized：同步加锁 JUC里面的lock：加锁 可见性 让一个线程对共享变量的修改对另一个线程可见 有序性 指令重排：处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的 要避免指令重排：volatile 线程池 线程池参数、原理 ⭐ 什么是线程池? 顾名思义，线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，**处理完之后线程并不会立即被销毁，而是等待下一个任务。**池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。 使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 1，任务在提交的时候，首先判断核心线程数是否已满，如果没有满则直接添加到工作线程执行 2，如果核心线程数满了，则判断阻塞队列是否已满，如果没有满，当前任务存入阻塞队列 3，如果阻塞队列也满了，则判断线程数是否小于最大线程数，如果满足条件，则使用临时线程执行任务 核心或临时线程执行完成任务后会检查阻塞队列中是否有需要执行的线程，如果有，则使用非核心线程执行任务 4，如果所有线程都在忙着（核心线程+临时线程），则走拒绝策略 拒绝策略 AbortPolicy：抛出RejectedExecutionException异常来拒绝新任务 CallerRunsPolicy：调用执行自己的线程运行任务，也就是在调用execute()方法的线程中运行被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。该策略会降低对于新任务的提交速度，影响程序的整体性能。 DiscardPolicy：不处理新任务，直接丢弃掉 DiscardOldestPolicy：丢弃掉最早未处理的任务 线程池常用的阻塞队列 比较常见的有4个，用的最多是ArrayBlockingQueue和LinkedBlockingQueue 1.ArrayBlockingQueue：基于数组结构的有界阻塞队列，FIFO。 2.LinkedBlockingQueue：基于链表结构的有界阻塞队列，FIFO。 3.DelayedWorkQueue ：延时队列，是一个优先级队列，可以实现定时任务，它可以保证每次出队的任务都是当前队列中执行时间最靠前的 4.SynchronousQueue：不存储元素的阻塞队列，每个插入操作都必须等待一个移出操作。 ArrayBlockingQueue的LinkedBlockingQueue区别 LinkedBlockingQueue ArrayBlockingQueue 默认无界，支持有界 强制有界 底层是链表 底层是数组 是懒惰的，创建节点的时候添加数据 提前初始化 Node 数组 入队会生成新 Node Node需要是提前创建好的 两把锁（头尾） 一把锁 左边是LinkedBlockingQueue加锁的方式，右边是ArrayBlockingQueue加锁的方式 LinkedBlockingQueue读和写各有一把锁，性能相对较好 ArrayBlockingQueue只有一把锁，读和写公用，性能相对于LinkedBlockingQueue差一些 如何确定核心线程数？ 在设置核心线程数之前，需要先熟悉一些执行线程池执行任务的类型 IO密集型任务 一般来说：文件读写、DB读写、网络请求等 推荐：核心线程数大小设置为 2N+1 （N为计算机的CPU核数） CPU密集型任务 一般来说：计算型代码、Bitmap转换、Gson转换等 推荐：核心线程数大小设置为 N+1 （N为计算机的CPU核数） 有一个简单并且适用面比较广的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 如何判断是 CPU 密集任务还是 IO 密集任务？ CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。 参考回答： ① 高并发、任务执行时间短 --&gt;（ CPU核数+1 ），减少线程上下文的切换 ② 并发不高、任务执行时间长 IO密集型的任务 --&gt; (CPU核数 * 2 + 1) 计算密集型任务 --&gt; （ CPU核数+1 ） ③ 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2） 线程池的种类 在java.util.concurrent.Executors类中提供了大量创建连接池的静态方法，常见就有四种 newFixedThreadPool 创建使用固定线程数的线程池 源码： public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);&#125; 核心线程数与最大线程数一样，没有救急线程 阻塞队列是LinkedBlockingQueue，最大容量为Integer.MAX_VALUE 适用场景：适用于任务量已知，相对耗时的任务 即使 maximumPoolSize 的值比 corePoolSize 大，也至多只会创建 corePoolSize 个线程。这是因为FixedThreadPool 使用的是容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue（无界队列），队列永远不会被放满。 public class FixedThreadPoolCase &#123; static class FixedThreadDemo implements Runnable&#123; @Override public void run() &#123; String name = Thread.currentThread().getName(); for (int i = 0; i &lt; 2; i++) &#123; System.out.println(name + &quot;:&quot; + i); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //创建一个固定大小的线程池，核心线程数和最大线程数都是3 ExecutorService executorService = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 5; i++) &#123; executorService.submit(new FixedThreadDemo()); Thread.sleep(10); &#125; executorService.shutdown(); &#125;&#125; newSingleThreadExecutor 单线程化的线程池，它只会用唯一的工作线程来执行任 务，保证所有任务按照指定顺序(FIFO)执行 源码 public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory)); &#125; 核心线程数和最大线程数都是1 阻塞队列是LinkedBlockingQueue，最大容量为Integer.MAX_VALUE 适用场景：适用于按照顺序执行的任务 SingleThreadExecutor 和 FixedThreadPool 一样，使用的都是容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue（无界队列）作为线程池的工作队列。SingleThreadExecutor 使用无界队列作为线程池的工作队列会对线程池带来的影响与 FixedThreadPool 相同。说简单点，就是可能会导致 OOM。 public class NewSingleThreadCase &#123; static int count = 0; static class Demo implements Runnable &#123; @Override public void run() &#123; count++; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + count); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //单个线程池，核心线程数和最大线程数都是1 ExecutorService exec = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; exec.execute(new Demo()); Thread.sleep(5); &#125; exec.shutdown(); &#125;&#125; newCachedThreadPool 可缓存线程池 源码 public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), threadFactory);&#125; 核心线程数为0 最大线程数是Integer.MAX_VALUE 阻塞队列为SynchronousQueue:不存储元素的阻塞队列，每个插入操作都必须等待一个移出操作。 适用场景：适合任务数比较密集，但每个任务执行时间较短的情况 CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。 public class CachedThreadPoolCase &#123; static class Demo implements Runnable &#123; @Override public void run() &#123; String name = Thread.currentThread().getName(); try &#123; //修改睡眠时间，模拟线程执行需要花费的时间 Thread.sleep(100); System.out.println(name + &quot;执行完了&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //创建一个缓存的线程，没有核心线程数，最大线程数为Integer.MAX_VALUE ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; exec.execute(new Demo()); Thread.sleep(1); &#125; exec.shutdown(); &#125;&#125; newScheduledThreadPool 提供了“延迟”和“周期执行”功能的ThreadPoolExecutor。 适用场景：有定时和延迟执行的任务 public class ScheduledThreadPoolCase &#123; static class Task implements Runnable &#123; @Override public void run() &#123; try &#123; String name = Thread.currentThread().getName(); System.out.println(name + &quot;, 开始：&quot; + new Date()); Thread.sleep(1000); System.out.println(name + &quot;, 结束：&quot; + new Date()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //按照周期执行的线程池，核心线程数为2，最大线程数为Integer.MAX_VALUE ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(2); System.out.println(&quot;程序开始：&quot; + new Date()); /** * schedule 提交任务到线程池中 * 第一个参数：提交的任务 * 第二个参数：任务执行的延迟时间 * 第三个参数：时间单位 */ scheduledThreadPool.schedule(new Task(), 0, TimeUnit.SECONDS); scheduledThreadPool.schedule(new Task(), 1, TimeUnit.SECONDS); scheduledThreadPool.schedule(new Task(), 5, TimeUnit.SECONDS); Thread.sleep(5000); // 关闭线程池 scheduledThreadPool.shutdown(); &#125;&#125; 为什么不建议用Executors创建线程池？ Executors创建的线程池，最大线程数等于核心线程数，并且请求队列无界，可能会造成OOM execute() vs submit() ⭐ execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法的话，如果在 timeout 时间内任务还没有执行完，就会抛出 java.util.concurrent.TimeoutException。 这里只是为了演示使用，推荐使用 ThreadPoolExecutor 构造方法来创建线程池。 示例 1：使用 get()方法获取返回值。 ExecutorService executorService = Executors.newFixedThreadPool(3);Future&lt;String&gt; submit = executorService.submit(() -&gt; &#123; try &#123; Thread.sleep(5000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;abc&quot;;&#125;);String s = submit.get();System.out.println(s);executorService.shutdown(); shutdown() VS shutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程池的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 调用完 shutdownNow 和 shuwdown 方法后，并不代表线程池已经完成关闭操作，它只是异步的通知线程池进行关闭处理。如果要同步等待线程池彻底关闭后才继续往下执行，需要调用awaitTermination方法进行同步等待。 在调用 awaitTermination() 方法时，应该设置合理的超时时间，以避免程序长时间阻塞而导致性能问题。另外。由于线程池中的任务可能会被取消或抛出异常，因此在使用 awaitTermination() 方法时还需要进行异常处理。awaitTermination() 方法会抛出 InterruptedException 异常，需要捕获并处理该异常，以避免程序崩溃或者无法正常退出 // ...// 关闭线程池executor.shutdown();try &#123; // 等待线程池关闭，最多等待5分钟 if (!executor.awaitTermination(5, TimeUnit.MINUTES)) &#123; // 如果等待超时，则打印日志 System.err.println(&quot;线程池未能在5分钟内完全关闭&quot;); &#125;&#125; catch (InterruptedException e) &#123; // 异常处理&#125; isTerminated() VS isShutdown() isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true 线程工厂 默认情况下创建的线程名字类似 pool-1-thread-n 这样的，没有业务含义，不利于我们定位问题。 给线程池里的线程命名通常有下面两种方式： 1、利用 guava 的 ThreadFactoryBuilder ThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat(threadNamePrefix + &quot;-%d&quot;) .setDaemon(true).build();ExecutorService threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.MINUTES, workQueue, threadFactory) 2、自己实现 ThreadFactory。 import java.util.concurrent.ThreadFactory;import java.util.concurrent.atomic.AtomicInteger;/** * 线程工厂，它设置线程名称，有利于我们定位问题。 */public final class NamingThreadFactory implements ThreadFactory &#123; private final AtomicInteger threadNum = new AtomicInteger(); private final String name; /** * 创建一个带名字的线程池生产工厂 */ public NamingThreadFactory(String name) &#123; this.name = name; &#125; @Override public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); t.setName(name + &quot; [#&quot; + threadNum.incrementAndGet() + &quot;]&quot;); return t; &#125;&#125; ThreadLocal 概述 ThreadLocal是多线程中对于解决线程安全的一个操作类，它会为每个线程都分配一个独立的线程副本从而解决了变量并发访问冲突的问题。ThreadLocal 同时实现了线程内的资源共享 案例：使用JDBC操作数据库时，会将每一个线程的Connection放入各自的ThreadLocal中，从而保证每个线程都在各自的 Connection 上进行数据库的操作，避免A线程关闭了B线程的连接。 ThreadLocal基本使用 三个主要方法： set(value) 设置值 get() 获取值 remove() 清除值 ThreadLocal的实现原理&amp;源码解析 从 Thread类源代码入手。 public class Thread implements Runnable &#123; //...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; //......&#125; 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是 null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。 ThreadLocal本质来说就是一个线程内部存储类，从而让多个线程只操作自己内部的值，从而实现线程数据隔离 在ThreadLocal中有一个内部类叫做ThreadLocalMap，类似于HashMap ThreadLocalMap中有一个属性table数组，这个是真正存储数据的位置 set方法 get方法/remove方法 通过上面这些内容，我们足以通过得出结论：最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。 实际上key并不是ThreadLocal本身，而是它的一个弱引用 ThreadLocal-内存泄露问题 Java对象中的四种引用类型：强引用、软引用、弱引用、虚引用 强引用：最为普通的引用方式，表示一个对象处于有用且必须的状态，如果一个对象具有强引用，则GC并不会回收它。即便堆中内存不足了，宁可出现OOM，也不会对其进行回收 弱引用：表示一个对象处于可能有用且非必须的状态。在GC线程扫描内存区域时，一旦发现弱引用，就会回收到弱引用相关联的对象。对于弱引用的回收，无关内存区域是否足够，一旦发现则会被回收 每一个Thread维护一个ThreadLocalMap，在ThreadLocalMap中的Entry对象继承了WeakReference。其中key为使用弱引用的ThreadLocal实例，value为线程变量的副本 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。 这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后最好手动调用remove()方法 在使用ThreadLocal的时候，强烈建议：务必手动remove 实际上： ThreadLocal的源码中设计了自动清理key为null的entry的代码逻辑。主要逻辑如下： 调用 set() 方法后，有以下几种情况： 情况1：通过 hash 计算后，Entry 数组对应的槽位为空，这种情况直接插入 key-value **情况2：**发生了哈希冲突，且槽位数据不为空，但槽位中的 key 与要插入的 key 相同，这种情况直接覆盖元数据。 情况3：发生了哈希冲突，槽位数据不为空，槽位中的key与当前要插入的key不同，则进行线性探测，向后进行查找： 情况3.1：探测过程中，遇到空Entry，则直接插入 **情况3.2：**探测过程中，遇到key相同的Entry，则直接覆盖 情况3.3：探测过程中，遇到 key = null 的槽位，槽位下标为 i，则调用 replaceStaleEntry() 方法，slotToExpunge = staleSlot = i，然后从当前位置向前进行探测，遇到key = null的槽位，则更新slotToExpunge的值，直到Entry为null，然后以 staleSlot 为起始位置向后进行探测： **情况3.3.1：**如果找到key相同的entry则直接覆盖，然后交换 table[i] 和table[staleSlot] 的数据 **情况3.3.2：**如果找到空槽，则直接插入，然后与table[staleSlot]交换位置 插入后，开始进行过期元素的清理。 清理过期元素的方法有：探测式清理、启发式清理 探测式清理从slotToExpunge开始向后清除过期元素，同时将没有过期的元素进行重新哈希，更新元素的位置，如果发生了哈冲突，则向后进行线性探测","tags":["java"],"categories":["后端"]},{"title":"about","path":"/about/index.html","content":"This is your last chance. After this there is no turning back. You take the blue pill, the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes… Remember, all I’m offering is the truth, nothing more… ​ --Morpheus , The Matrix"}]